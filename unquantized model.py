import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import Settings
documents = SimpleDirectoryReader("/content/data").load_data() #path to the data


from llama_index.embeddings.fastembed import FastEmbedEmbedding
embed_model = FastEmbedEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.embed_model = embed_model
Settings.chunk_size = 512 #change as you wish


from llama_index.core import PromptTemplate
system_prompt = "You are an assistant. Answer questions as accurately as possible based on the instructions and context provided."
# This will wrap the default prompts that are internal to llama-index
query_wrapper_prompt = PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")


#Gemma needs you to sign an agrement
from huggingface_hub import  notebook_login
notebook_login()


import torch

model = HuggingFaceLLM(
    context_window=8192, #for chat history and flow of dialog
    max_new_tokens=256,  #max tokens generated by model for each response
    generate_kwargs={"temperature": 0.7, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="google/gemma-2b-it",
    model_name="google/gemma-2b-it",
    device_map="auto",
    model_kwargs={"torch_dtype": torch.float16}
)

Settings.model = model
Settings.chunk_size = 512


index = VectorStoreIndex.from_documents(documents)


query_engine = index.as_query_engine()
def predict(input, history):
  response = query_engine.query(input)
  return str(response)

import gradio as gr  #prebult UI, you can remove this if you are making your own
gr.ChatInterface(predict).launch(share=True)
